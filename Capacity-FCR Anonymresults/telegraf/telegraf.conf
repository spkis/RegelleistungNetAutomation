[global_tags]

[agent]
  interval = "5s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "10s"
  precision = "1ms"
  #debug = true
  # quiet = false
  # logtarget = "file"
  #logfile = "/var/log/telegraf/debug.log"
  hostname = ""
  omit_hostname = true

[[outputs.file]]
  files = ["/tmp/telegraf/metrics.out"]
  data_format = "influx"

[[outputs.influxdb_v2]]
  urls = ["http://192.168.1.100:8086"]
  token = "TOKEN"
  organization = "ju:niz"
  bucket = "regelleistung_net"
  # bucket_tag = ""
  # exclude_bucket_tag = false
  # timeout = "5s"
  # http_headers = {"X-Special-Header" = "Special-Value"}
  # http_proxy = "http://corporate.proxy:3128"
  # user_agent = "telegraf"
  # content_encoding = "gzip"
  ## Enable or disable uint support for writing uints influxdb 2.0.
  influx_uint_support = true

  ## Optional TLS Config for use on HTTP connections.
  # tls_ca = "/etc/telegraf/ca.pem"
  # tls_cert = "/etc/telegraf/cert.pem"
  # tls_key = "/etc/telegraf/key.pem"
  ## Use TLS but skip chain & host verification
  #insecure_skip_verify = false



# Ingests files in a directory and then moves them to a target directory.
[[inputs.directory_monitor]]
  ## The directory to monitor and read files from (including sub-directories if "recursive" is true).
  directory = "../telegraf_import"
  #
  ## The directory to move finished files to (maintaining directory hierachy from source).
  finished_directory = "../telegraf_import/success"
  #
  ## Setting recursive to true will make the plugin recursively walk the directory and process all sub-directories.
  # recursive = false
  #
  ## The directory to move files to upon file error.
  ## If not provided, erroring files will stay in the monitored directory.
  # error_directory = ""
  #
  ## The amount of time a file is allowed to sit in the directory before it is picked up.
  ## This time can generally be low but if you choose to have a very large file written to the directory and it's potentially slow,
  ## set this higher so that the plugin will wait until the file is fully copied to the directory.
  # directory_duration_threshold = "50ms"
  #
  ## A list of the only file names to monitor, if necessary. Supports regex. If left blank, all files are ingested.
  # files_to_monitor = ["^.*\.csv"]
  #
  ## A list of files to ignore, if necessary. Supports regex.
  # files_to_ignore = [".DS_Store"]
  #
  ## Maximum lines of the file to process that have not yet be written by the
  ## output. For best throughput set to the size of the output's metric_buffer_limit.
  ## Warning: setting this number higher than the output's metric_buffer_limit can cause dropped metrics.
  # max_buffered_metrics = 10000
  #
  ## The maximum amount of file paths to queue up for processing at once, before waiting until files are processed to find more files.
  ## Lowering this value will result in *slightly* less memory use, with a potential sacrifice in speed efficiency, if absolutely necessary.
  # file_queue_size = 100000
  #
  ## Name a tag containing the name of the file the data was parsed from.  Leave empty
  ## to disable. Cautious when file name variation is high, this can increase the cardinality
  ## significantly. Read more about cardinality here:
  ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality
  # file_tag = ""
  #
  ## Specify if the file can be read completely at once or if it needs to be read line by line (default).
  ## Possible values: "line-by-line", "at-once"
  # parse_method = "line-by-line"
  #
  ## The dataformat to be read from the files.
  ## Each data format has its own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
  data_format = "csv"



  ## Indicates how many rows to treat as a header. By default, the parser assumes
  ## there is no header and will parse the first row as data. If set to anything more
  ## than 1, column names will be concatenated with the name listed in the next header row.
  ## If `csv_column_names` is specified, the column names in header will be overridden.
  csv_header_row_count = 1

  ## For assigning custom names to columns
  ## If this is specified, all columns should have a name
  ## Unnamed columns will be ignored by the parser.
  ## If `csv_header_row_count` is set to 0, this config must be used
  # csv_column_names = ["DATE_FROM","DATE_TO","PRODUCT_TYPE","TENDER_NUMBER","PRODUCTNAME","CROSSBORDER_SETTLEMENTCAPACITY_PRICE_[EUR/MW]","AUSTRIA_DEMAND_[MW]","AUSTRIA_SETTLEMENTCAPACITY_PRICE_[EUR/MW]","AUSTRIA_DEFICIT(-)_SURPLUS(+)_[MW]","BELGIUM_DEMAND_[MW]","BELGIUM_SETTLEMENTCAPACITY_PRICE_[EUR/MW]","BELGIUM_DEFICIT(-)_SURPLUS(+)_[MW]","DENMARK_DEMAND_[MW]","DENMARK_SETTLEMENTCAPACITY_PRICE_[EUR/MW]","DENMARK_DEFICIT(-)_SURPLUS(+)_[MW]","FRANCE_DEMAND_[MW]","FRANCE_SETTLEMENTCAPACITY_PRICE_[EUR/MW]","FRANCE_DEFICIT(-)_SURPLUS(+)_[MW]","GERMANY_DEMAND_[MW]","GERMANY_SETTLEMENTCAPACITY_PRICE_[EUR/MW]","GERMANY_DEFICIT(-)_SURPLUS(+)_[MW]","NETHERLANDS_DEMAND_[MW]","NETHERLANDS_SETTLEMENTCAPACITY_PRICE_[EUR/MW]","NETHERLANDS_DEFICIT(-)_SURPLUS(+)_[MW]","SLOVENIA_DEMAND_[MW]","SLOVENIA_SETTLEMENTCAPACITY_PRICE_[EUR/MW]","SLOVENIA_DEFICIT(-)_SURPLUS(+)_[MW]","SWITZERLAND_DEMAND_[MW]","SWITZERLAND_SETTLEMENTCAPACITY_PRICE_[EUR/MW]","SWITZERLAND_DEFICIT(-)_SURPLUS(+)_[MW]","CZECH_REPUBLIC_DEMAND_[MW]","CZECH_REPUBLIC_SETTLEMENTCAPACITY_PRICE_[EUR/MW]","CZECH_REPUBLIC_DEFICIT(-)_SURPLUS(+)_[MW]"]

  ## For assigning explicit data types to columns.
  ## Supported types: "int", "float", "bool", "string".
  ## Specify types in order by column (e.g. `["string", "int", "float"]`)
  ## If this is not specified, type conversion will be done on the types above.
  #csv_column_types = []

  ## Indicates the number of rows to skip before looking for metadata and header information.
  csv_skip_rows = 0

  ## Indicates the number of rows to parse as metadata before looking for header information.
  ## By default, the parser assumes there are no metadata rows to parse.
  ## If set, the parser would use the provided separators in the csv_metadata_separators to look for metadata.
  ## Please note that by default, the (key, value) pairs will be added as tags.
  ## If fields are required, use the converter processor.
  csv_metadata_rows = 0

  ## A list of metadata separators. If csv_metadata_rows is set,
  ## csv_metadata_separators must contain at least one separator.
  ## Please note that separators are case sensitive and the sequence of the seperators are respected.
  csv_metadata_separators = [":", "="]

  ## A set of metadata trim characters.
  ## If csv_metadata_trim_set is not set, no trimming is performed.
  ## Please note that the trim cutset is case sensitive.
  csv_metadata_trim_set = ""

  ## Indicates the number of columns to skip before looking for data to parse.
  ## These columns will be skipped in the header as well.
  csv_skip_columns = 0

  ## The separator between csv fields
  ## By default, the parser assumes a comma (",")
  ## Please note that if you use invalid delimiters (e.g. "\u0000"), commas
  ## will be changed to "\ufffd", the invalid delimiters changed to a comma
  ## during parsing, and afterwards the invalid characters and commas are
  ## returned to their original values.
  csv_delimiter = ","

  ## The character reserved for marking a row as a comment row
  ## Commented rows are skipped and not parsed
  csv_comment = ""

  ## If set to true, the parser will remove leading whitespace from fields
  ## By default, this is false
  csv_trim_space = false

  ## Columns listed here will be added as tags. Any other columns
  ## will be added as fields.
  csv_tag_columns = ["TYPE_OF_RESERVES","PRODUCTNAME"]

  ## Set to true to let the column tags overwrite the metadata and default tags.
  csv_tag_overwrite = false

  ## The column to extract the name of the metric from. Will not be
  ## included as field in metric.
  csv_measurement_column = ""

  ## The column to extract time information for the metric
  ## `csv_timestamp_format` must be specified if this is used.
  ## Will not be included as field in metric.
  csv_timestamp_column = ""

  ## The format of time data extracted from `csv_timestamp_column`
  ## this must be specified if `csv_timestamp_column` is specified
  csv_timestamp_format = ""

  ## The timezone of time data extracted from `csv_timestamp_column`
  ## in case of there is no timezone information.
  ## It follows the  IANA Time Zone database.
  csv_timezone = ""

  ## Indicates values to skip, such as an empty string value "".
  ## The field will be skipped entirely where it matches any values inserted here.
  csv_skip_values = []

  ## If set to true, the parser will skip csv lines that cannot be parsed.
  ## By default, this is false
  csv_skip_errors = false

  ## Reset the parser on given conditions.
  ## This option can be used to reset the parser's state e.g. when always reading a
  ## full CSV structure including header etc. Available modes are
  ##    "none"   -- do not reset the parser (default)
  ##    "always" -- reset the parser with each call (ignored in line-wise parsing)
  ##                Helpful when e.g. reading whole files in each gather-cycle.
  # csv_reset_mode = "none"
